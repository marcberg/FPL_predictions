{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('artifacts/train.csv')\n",
    "test_df=pd.read_csv('artifacts/test.csv')\n",
    "val_df=pd.read_csv('artifacts/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_df, test_df]).drop(columns=['label_1', 'label_X', 'label_2'], axis=1).reset_index(drop=True)\n",
    "y_train = pd.concat([train_df, test_df])[['label_1', 'label_X', 'label_2']].reset_index(drop=True)\n",
    "\n",
    "X_val = val_df.drop(columns=['label_1', 'label_X', 'label_2'], axis=1).reset_index(drop=True)\n",
    "y_val = val_df[['label_1', 'label_X', 'label_2']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863, 59)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcus Bergdahl\\AppData\\Local\\Temp\\ipykernel_1264\\2565299464.py:12: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step - loss: 23.6989 - accuracy: 0.4035\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 9.5340 - accuracy: 0.4017\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 6.7368 - accuracy: 0.4104\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 4.9384 - accuracy: 0.4243\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 5.1138 - accuracy: 0.4278\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.7135 - accuracy: 0.4696\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.2144 - accuracy: 0.4452\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.7645 - accuracy: 0.5148\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.8681 - accuracy: 0.4870\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.4248 - accuracy: 0.5061\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 5.2097 - accuracy: 0.4479\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 28.1201 - accuracy: 0.3965\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 12.0595 - accuracy: 0.4174\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.1237 - accuracy: 0.3948\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.7390 - accuracy: 0.4243\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0826 - accuracy: 0.4243\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9252 - accuracy: 0.4800\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.4825 - accuracy: 0.4365\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.0459 - accuracy: 0.4713\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.2252 - accuracy: 0.4626\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.7816 - accuracy: 0.4904\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 4.9759 - accuracy: 0.4062\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 29.8929 - accuracy: 0.3507\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 12.3688 - accuracy: 0.4427\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.6507 - accuracy: 0.3750\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.6984 - accuracy: 0.4080\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.4737 - accuracy: 0.4549\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.3889 - accuracy: 0.4497\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.8370 - accuracy: 0.4757\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.9208 - accuracy: 0.4688\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.1997 - accuracy: 0.4549\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.0721 - accuracy: 0.4514\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 7.6766 - accuracy: 0.3066\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 1s 2ms/step - loss: 168.3891 - accuracy: 0.4174\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.6256 - accuracy: 0.4557\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1785 - accuracy: 0.4817\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0754 - accuracy: 0.5043\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1595 - accuracy: 0.4800\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0757 - accuracy: 0.4661\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0474 - accuracy: 0.4574\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0639 - accuracy: 0.4678\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0538 - accuracy: 0.4609\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.0556 - accuracy: 0.4730\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2408 - accuracy: 0.3750\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 679.3477 - accuracy: 0.4104\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.3511 - accuracy: 0.4330\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.2998 - accuracy: 0.4348\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.1230 - accuracy: 0.4226\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.1109 - accuracy: 0.4383\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0754 - accuracy: 0.4313\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0710 - accuracy: 0.4435\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0557 - accuracy: 0.4470\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1105 - accuracy: 0.4470\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0684 - accuracy: 0.4400\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.4376 - accuracy: 0.4375\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 139.9463 - accuracy: 0.3976\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 10.5830 - accuracy: 0.3976\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.3707 - accuracy: 0.4167\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1663 - accuracy: 0.4132\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.1060 - accuracy: 0.4253\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0773 - accuracy: 0.4306\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0783 - accuracy: 0.4427\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0715 - accuracy: 0.4410\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0650 - accuracy: 0.4549\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.1187 - accuracy: 0.4618\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.1158 - accuracy: 0.4425\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 30.4795 - accuracy: 0.3583\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 11.0308 - accuracy: 0.3878\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.0028 - accuracy: 0.3896\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.6703 - accuracy: 0.4365\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1231 - accuracy: 0.4487\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.5200 - accuracy: 0.4417\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.6274 - accuracy: 0.4696\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5447 - accuracy: 0.4661\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6081 - accuracy: 0.4870\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4222 - accuracy: 0.4574\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 6.6051 - accuracy: 0.4167\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 25.8997 - accuracy: 0.3635\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.4724 - accuracy: 0.3965\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.6407 - accuracy: 0.3652\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.7480 - accuracy: 0.3861\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1187 - accuracy: 0.4243\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 4.3878 - accuracy: 0.4070\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.8967 - accuracy: 0.4435\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5882 - accuracy: 0.4626\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.9402 - accuracy: 0.4609\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6554 - accuracy: 0.4713\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 5.2620 - accuracy: 0.3854\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 27.6124 - accuracy: 0.3924\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.8074 - accuracy: 0.4010\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 5.1806 - accuracy: 0.4253\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 4.0198 - accuracy: 0.4323\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.0108 - accuracy: 0.4375\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.7164 - accuracy: 0.4722\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.4814 - accuracy: 0.4757\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2621 - accuracy: 0.4740\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.2187 - accuracy: 0.4913\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.8407 - accuracy: 0.4826\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.7599 - accuracy: 0.3972\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 56.6035 - accuracy: 0.4122\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1223 - accuracy: 0.4504\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0931 - accuracy: 0.4504\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0886 - accuracy: 0.4504\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0847 - accuracy: 0.4504\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0817 - accuracy: 0.4504\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0794 - accuracy: 0.4504\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0776 - accuracy: 0.4504\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0756 - accuracy: 0.4504\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0740 - accuracy: 0.4504\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0937 - accuracy: 0.3681\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 9517.7930 - accuracy: 0.3513\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 11476.0117 - accuracy: 0.3809\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0923 - accuracy: 0.4035\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0909 - accuracy: 0.4035\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0895 - accuracy: 0.4035\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0884 - accuracy: 0.4035\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0873 - accuracy: 0.4035\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0865 - accuracy: 0.4035\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0857 - accuracy: 0.4035\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0850 - accuracy: 0.4035\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 187.2195 - accuracy: 0.4653\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2538332.0000 - accuracy: 0.3299\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2214484957497693044736.0000 - accuracy: 0.3802\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 4370341210621346840576.0000 - accuracy: 0.3490\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0943 - accuracy: 0.3802\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0920 - accuracy: 0.4184\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0899 - accuracy: 0.4184\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.0880 - accuracy: 0.4184\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0863 - accuracy: 0.4184\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0849 - accuracy: 0.4184\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0836 - accuracy: 0.4184\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.1525 - accuracy: 0.4321\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 1s 2ms/step - loss: 30.5615 - accuracy: 0.2748\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.9338 - accuracy: 0.4035\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1712 - accuracy: 0.4383\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.3057 - accuracy: 0.4000\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.6258 - accuracy: 0.3983\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.1988 - accuracy: 0.4174\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.0074 - accuracy: 0.4278\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.9193 - accuracy: 0.4557\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.6301 - accuracy: 0.4557\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.4824 - accuracy: 0.4643\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.6110 - accuracy: 0.4444\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 30.8446 - accuracy: 0.3739\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.6153 - accuracy: 0.3322\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.9556 - accuracy: 0.3757\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.7644 - accuracy: 0.3913\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2320 - accuracy: 0.4052\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.9424 - accuracy: 0.4052\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.7412 - accuracy: 0.4383\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.5200 - accuracy: 0.4243\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.4058 - accuracy: 0.4348\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.3733 - accuracy: 0.4417\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.0435 - accuracy: 0.3819\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 25.1062 - accuracy: 0.4062\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 10.4008 - accuracy: 0.4028\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.8850 - accuracy: 0.3889\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 2.4371 - accuracy: 0.3611\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.9033 - accuracy: 0.3681\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.5244 - accuracy: 0.4201\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.2582 - accuracy: 0.4306\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1226 - accuracy: 0.4306\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.1018 - accuracy: 0.4271\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0661 - accuracy: 0.4392\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.7774 - accuracy: 0.3693\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 704102656.0000 - accuracy: 0.4139\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0926 - accuracy: 0.4504\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0891 - accuracy: 0.4504\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0861 - accuracy: 0.4504\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0833 - accuracy: 0.4504\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0808 - accuracy: 0.4504\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0786 - accuracy: 0.4504\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0767 - accuracy: 0.4504\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0750 - accuracy: 0.4504\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0736 - accuracy: 0.4504\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.0915 - accuracy: 0.3681\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.3870 - accuracy: 0.4035\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0960 - accuracy: 0.4052\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0940 - accuracy: 0.4052\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0919 - accuracy: 0.4087\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0891 - accuracy: 0.4087\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0868 - accuracy: 0.4139\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0850 - accuracy: 0.4139\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0826 - accuracy: 0.4191\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0803 - accuracy: 0.4174\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.0780 - accuracy: 0.4313\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0849 - accuracy: 0.4306\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 1s 2ms/step - loss: 19125.8652 - accuracy: 0.3906\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0946 - accuracy: 0.4184\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0922 - accuracy: 0.4184\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0901 - accuracy: 0.4184\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0882 - accuracy: 0.4184\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0865 - accuracy: 0.4184\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0850 - accuracy: 0.4184\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0837 - accuracy: 0.4184\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.0825 - accuracy: 0.4184\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 1.0815 - accuracy: 0.4184\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0817 - accuracy: 0.4321\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 270.1373 - accuracy: 0.3893\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 6.4079 - accuracy: 0.4090\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.3687 - accuracy: 0.4102\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.1642 - accuracy: 0.4067\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.1277 - accuracy: 0.4102\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0969 - accuracy: 0.4137\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0855 - accuracy: 0.4160\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0762 - accuracy: 0.4171\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 1.0791 - accuracy: 0.4148\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 1.0764 - accuracy: 0.4183\n",
      "Best Hyperparameters:  {'hidden_units': (64, 32), 'optimizer': 'sgd'}\n",
      "Best Accuracy:  0.42179636160532635\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "Validation Predictions:\n",
      "[[0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4042713  0.25156066 0.344168  ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.443106   0.2955525  0.26134157]\n",
      " [0.35482514 0.30540445 0.33977035]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.2223521  0.26960424 0.50804365]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41204807 0.2415348  0.3464172 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.45154336 0.30751535 0.24094138]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.28887182 0.30760315 0.40352497]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.45209926 0.30834103 0.23955962]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.453676   0.31071168 0.23561235]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.3323033  0.25062895 0.41706777]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.39729893 0.246177   0.35652408]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.38826603 0.2991728  0.31256115]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.43500683 0.2849079  0.2800853 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.52281004 0.2972912  0.17989877]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.44120514 0.32913756 0.22965728]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.39193204 0.37907273 0.22899523]\n",
      " [0.38490438 0.26237997 0.35271558]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.37176543 0.2761189  0.35211566]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4619729  0.32399312 0.21403404]\n",
      " [0.30870008 0.37893102 0.31236896]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4169714  0.26336598 0.31966266]\n",
      " [0.48107728 0.27048975 0.24843289]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4222072  0.26936078 0.30843204]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.38375455 0.28042367 0.3358218 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.44528887 0.29855263 0.25615853]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41886383 0.26551113 0.31562507]\n",
      " [0.3674349  0.33991075 0.29265442]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4970558  0.30250862 0.20043562]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41631123 0.2626232  0.3210656 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4563448  0.3148271  0.22882801]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41532007 0.26151335 0.3231666 ]\n",
      " [0.40661103 0.2534475  0.3399416 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.40985414 0.36172712 0.2284187 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41302705 0.32970238 0.25727057]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4493168  0.3042579  0.24642536]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]]\n"
     ]
    }
   ],
   "source": [
    "# Define your deep learning model with increased complexity\n",
    "def create_model(hidden_units=[128, 64, 32], optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units[0], input_shape=(X.shape[1],), activation='relu'))\n",
    "    for units in hidden_units[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'hidden_units': [(128,), (64, 32), (32, 16, 8)]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=parameters, cv=3)\n",
    "\n",
    "# Perform grid search on your training data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n",
      "Validation Predictions:\n",
      "[[0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4042713  0.25156066 0.344168  ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.443106   0.2955525  0.26134157]\n",
      " [0.35482514 0.30540445 0.33977035]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.2223521  0.26960424 0.50804365]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41204807 0.2415348  0.3464172 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.45154336 0.30751535 0.24094138]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.28887182 0.30760315 0.40352497]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.45209926 0.30834103 0.23955962]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.453676   0.31071168 0.23561235]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.3323033  0.25062895 0.41706777]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.39729893 0.246177   0.35652408]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.38826603 0.2991728  0.31256115]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.43500683 0.2849079  0.2800853 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.52281004 0.2972912  0.17989877]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.44120514 0.32913756 0.22965728]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.39193204 0.37907273 0.22899523]\n",
      " [0.38490438 0.26237997 0.35271558]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.37176543 0.2761189  0.35211566]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4619729  0.32399312 0.21403404]\n",
      " [0.30870008 0.37893102 0.31236896]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4169714  0.26336598 0.31966266]\n",
      " [0.48107728 0.27048975 0.24843289]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4222072  0.26936078 0.30843204]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.38375455 0.28042367 0.3358218 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.44528887 0.29855263 0.25615853]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41886383 0.26551113 0.31562507]\n",
      " [0.3674349  0.33991075 0.29265442]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4970558  0.30250862 0.20043562]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41631123 0.2626232  0.3210656 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4563448  0.3148271  0.22882801]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41532007 0.26151335 0.3231666 ]\n",
      " [0.40661103 0.2534475  0.3399416 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.40985414 0.36172712 0.2284187 ]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41302705 0.32970238 0.25727057]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.4493168  0.3042579  0.24642536]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]\n",
      " [0.41772482 0.24013184 0.34214336]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "result = best_model.predict_proba(X_val)\n",
    "print(\"Validation Predictions:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
